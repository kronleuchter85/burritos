{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import time , datetime , date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import psycopg2\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lectura y escritura de dataframes a archivos Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "pickle_storage = './dataframes/'\n",
    "pickle_extension = '.pkl'\n",
    "\n",
    "def get_pickle(dbname,name):\n",
    "    filename = pickle_storage + dbname + '_'+ name + pickle_extension\n",
    "    return pd.read_pickle(filename)\n",
    "\n",
    "def put_pickle(dbname,name, frame):\n",
    "    filename = pickle_storage  + dbname + '_'+ name + pickle_extension\n",
    "    frame.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectando con MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     17,
     21,
     31,
     39,
     44,
     49,
     57,
     77,
     80,
     90
    ]
   },
   "outputs": [],
   "source": [
    "class LabelMetadata:\n",
    "    intervals = None\n",
    "    minDate = None\n",
    "    maxDate = None\n",
    "    minTime = None\n",
    "    maxTime = None\n",
    "    outputName = None\n",
    "    table = None\n",
    "    labelName = None\n",
    "    requireAdvances = False\n",
    "    \n",
    "    def getMinDateTime(self):\n",
    "        return get_date_time(self.minDate , self.minTime)\n",
    "    \n",
    "    def getMaxDateTime(self):\n",
    "        return get_date_time(self.maxDate , self.maxTime)\n",
    "    \n",
    "def get_date_time(date,time):\n",
    "    pattern = '%Y-%m-%d %H:%M:%S'\n",
    "    return datetime.strptime(date + ' ' + time , pattern)\n",
    "\n",
    "def get_interval_description(i):\n",
    "    text = i['text']\n",
    "    \n",
    "    max_x = i['x_max']\n",
    "    min_x = i['x_min']\n",
    "    points = i['points']\n",
    "    points = map(lambda p: ( str(p['x']) , float(p['y']) ) , points)\n",
    "    \n",
    "    return (str(min_x) , str(max_x))\n",
    "\n",
    "def get_metadata_object(name):\n",
    "    labels_collection = db.labels\n",
    "    labels_cursor = labels_collection.find({\"name\": name})\n",
    "    elements = []\n",
    "    for l in labels_cursor:\n",
    "        elements.append(l)\n",
    "    return elements[0]\n",
    "\n",
    "def get_label_intervals_by_name(name):\n",
    "    metadata_element = get_metadata_object(name)\n",
    "    intervals = metadata_element['intervals']\n",
    "    return intervals;\n",
    "\n",
    "def get_datetime_pairs(x):\n",
    "    d1 = datetime(x[0].year, x[0].month, x[0].day, x[0].hour, x[0].minute, x[0].second )\n",
    "    d2 = datetime(x[1].year, x[1].month, x[1].day, x[1].hour, x[1].minute, x[1].second )\n",
    "    return (d1,d2)\n",
    "\n",
    "def get_labels_by_name(intervals):\n",
    "    interval_descriptions = map(lambda i : get_interval_description(i) , intervals)\n",
    "    pattern = '%Y-%m-%d %H:%M:%S.%f'\n",
    "    interval_descriptions= map( lambda x: (datetime.strptime(x[0],pattern) , datetime.strptime(x[1] , pattern))  \n",
    "                               ,interval_descriptions)\n",
    "    \n",
    "    return map(lambda x: get_datetime_pairs(x) ,interval_descriptions)\n",
    "    \n",
    "def get_metadata(name):\n",
    "    \n",
    "    ## mongo metadata stored\n",
    "    metadata_element = get_metadata_object(name)\n",
    "    \n",
    "    metadata = LabelMetadata()\n",
    "    metadata.intervals = get_labels_by_name(metadata_element['intervals'])\n",
    "    metadata.table = str(metadata_element['table'])\n",
    "    metadata.minDate = str(metadata_element['minDate'])\n",
    "    metadata.maxDate = str(metadata_element['maxDate'])\n",
    "    metadata.minTime = str(metadata_element['minTime'])\n",
    "    metadata.maxTime = str(metadata_element['maxTime'])\n",
    "    metadata.labelName = str(metadata_element['name'])\n",
    "    metadata.outputName = str(metadata_element['outputName'])\n",
    "    metadata.minLimit = get_date_time(metadata.minDate , metadata.minTime)\n",
    "    metadata.maxLimit = get_date_time(metadata.maxDate , metadata.maxTime)\n",
    "    metadata.requireAdvances = bool(metadata_element['requireAdvances'])\n",
    "    \n",
    "    return metadata\n",
    "    \n",
    "def get_mongo_data_frame(name):\n",
    "    return pd.DataFrame( list( db[ name ].find() ))\n",
    "\n",
    "def convert_fields_to_datetime(df,fields):\n",
    "    df = df.copy()\n",
    "    \n",
    "    for f in fields:\n",
    "        col_vals = df[f].values\n",
    "        new_col_vals = map(lambda x : pd.Timestamp(x).to_pydatetime() , col_vals)\n",
    "        df[f] = new_col_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "def put_mongo_data_frame(name , dataframe):\n",
    "    db.drop_collection(name)\n",
    "    db[name].insert_many(dataframe.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conectando con PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0,
     8,
     44,
     52,
     57
    ]
   },
   "outputs": [],
   "source": [
    "def connect(host , db):\n",
    "    #conn_string = \"host='192.168.1.3' dbname='gtrader' user='postgres' password='postgres'\"\n",
    "    conn_string = \"host='\" + host +\"' dbname='\" + db + \"' user='postgres' password='postgres'\"\n",
    "    print \"Connecting to database:\",  (conn_string)\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    print \"Connected!\"\n",
    "    return conn\n",
    "\n",
    "def analyze(prod_name, df  , features ):\n",
    "\n",
    "    data = []\n",
    "   \n",
    "    ff = go.Scatter(\n",
    "        x=df.index,\n",
    "        y=df[features],\n",
    "        name = prod_name,\n",
    "        line = dict(color = '#17BECF'),\n",
    "        opacity = 0.8)\n",
    "    data.append(ff)\n",
    "   \n",
    "    layout = dict(\n",
    "        title=prod_name,\n",
    "        xaxis=dict(\n",
    "            rangeselector=dict(\n",
    "                buttons=list([\n",
    "                    dict(count=1,\n",
    "                         label='1m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(count=6,\n",
    "                         label='6m',\n",
    "                         step='month',\n",
    "                         stepmode='backward'),\n",
    "                    dict(step='all')\n",
    "                ])\n",
    "            ),\n",
    "            rangeslider=dict(),\n",
    "            type='date'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = dict(data=data, layout=layout)\n",
    "    off.plot(fig , filename= prod_name + '.html')\n",
    "\n",
    "def get_frame_by_date_range(table,minDate,minTime,maxDate,maxTime):\n",
    "    query = queryForRange.replace('@table' , table)\n",
    "    query = query.replace('@minDate' , minDate )\n",
    "    query = query.replace('@minTime' , minTime )\n",
    "    query = query.replace('@maxDate' , maxDate )\n",
    "    query = query.replace('@maxTime' , maxTime )\n",
    "    return get_frame(query)\n",
    "    \n",
    "def get_frame(query ):\n",
    "    df = pd.read_sql( query ,conn)\n",
    "    print 'Leidos:',len(df)\n",
    "    return df\n",
    "\n",
    "def clean_data(frame):\n",
    "    frame2 = frame.copy()\n",
    "\n",
    "    frame2['datetime'] = frame2.apply(lambda x : datetime.combine( x['event_date'] , x['event_time'] ) , axis = 1)\n",
    "    frame2['event_time'] = frame2['event_time'].apply(lambda x : time.strftime(x , '%H:%M:%S.%f'))\n",
    "    frame2['event_date'] = frame2['event_date'].apply(lambda x : date.strftime(x , '%Y-%m-%d'))\n",
    "    frame2 = frame2.set_index(['datetime'])\n",
    "    \n",
    "    return frame2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to database: host='localhost' dbname='ds' user='postgres' password='postgres'\n",
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "conn = connect('localhost' , 'ds')\n",
    "#conn = connect('192.168.1.3' , 'gtrader')\n",
    "queryForRange = \"select * from @table where (event_date + event_time) between (date '@minDate' + time '@minTime' ) and ( date '@maxDate' + time '@maxTime')\";\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo - Eventos estudiados\n",
    "\n",
    "<ul>\n",
    "    <li>Spread</li>\n",
    "\n",
    "    <li>Demmand Buys p/ intervalo</li>\n",
    "    <li>Demmand Sells p/ intervalo</li>\n",
    "    <li>Demmand Buys totales</li>\n",
    "    <li>Demmand Sells totales</li>\n",
    "    \n",
    "    <li>Absorciones (Bloqueos): Compras/Ventas hechas por la demanda que no logran comer el nivel de la oferta sobre el que \n",
    "        impactan. Por lo tanto la siguiente operacion del mismo tipo impacta en el mismo nivel de la oferta.\n",
    "        <ul>\n",
    "            <li>Absorciones Ask p/ intervalo</li>\n",
    "            <li>Absorciones Ask totales</li>\n",
    "            <li>Absorciones Bid p/ intervalo</li>\n",
    "            <li>Absorciones Bid totales</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Rupturas: Compras/Ventas hechas por la demanda que SI logran comer el nivel de la oferta sobre el que \n",
    "        impactan. Por lo tanto la siguiente operacion del mismo tipo impacta en un nivel superior/inferior de la oferta.\n",
    "        <ul>\n",
    "            <li>Rupturas Ask p/ intervalo</li>\n",
    "            <li>Rupturas Ask totales</li>\n",
    "            <li>Rupturas Bid p/ intervalo</li>\n",
    "            <li>Rupturas Bid totales</li>            \n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Contra-ataques: Avances de la oferta en un sentido.\n",
    "        <ul>\n",
    "            <li>Contra-ataques Ask p/ intervalo</li>\n",
    "            <li>Contra-ataques Ask totales</li>\n",
    "            <li>Contra-ataques Bid p/ intervalo</li>\n",
    "            <li>Contra-ataques Bid totales</li>\n",
    "        </ul>\n",
    "    </li>   \n",
    "    <li>Retiradas: Retrocesos de la oferta en un sentido.\n",
    "        <ul>\n",
    "            <li>Retiradas Ask p/ intervalo</li>\n",
    "            <li>Retiradas Ask totales</li>\n",
    "            <li>Retiradas Bid p/ intervalo</li>\n",
    "            <li>Retiradas Bid totales</li>\n",
    "        </ul>\n",
    "    </li>  \n",
    "    \n",
    "    \n",
    "    <li>Unknown counts p/ intervalo</li>\n",
    "    <li>Unknown counts p/ totales</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitacora\n",
    "\n",
    "Otras combinaciones disponibles como puntos de mejora\n",
    "<lu>\n",
    "   \n",
    "     <li>Considerar distintos numeros de niveles del book</li>\n",
    "     <li>Distintos humbrales de comienzo y fin de cada tramo</li>\n",
    "    <li>Agregar mas de dos intervalos (solo si se puede reconocer algun label para un estado intermedio)</li>\n",
    "    <li>Variar la regularizacion</li>\n",
    "    <li>Hacer una regresion del ultimo estado para calcular su variacion promedio</li>\n",
    "    <li>Probar para diferentes modelos clasificatorios, por ejemplo RNAs, Arboles, etc</li>\n",
    "    <li>Los tramos estacionarios pueden tener una componente de tendencia. Se puede por medio de una regresion lineal determinar los coeficientes de la misma.</li>\n",
    "    <li>Dentro de los tramos estacionarios puede haber varios cambios de pendiente, por lo cual si se construyera una RL para detrendearlas seria necesario calcularla por subintervalos</li>\n",
    " ----------------------------------\n",
    "  <li>Listo: Variar el tamaño de las ventanas de tiempo para obtener distintas frecuencias</li>\n",
    "    <li>Listo: Variar los features intervinientes en el modelo</li>\n",
    "<li>Listo: Variar los porcentajes de avance de los intervalos (ej. 70-30 , 80-20 , 60-40 , etc)</li>\n",
    "<li>Listo: Probar diferentes algoritmos</li>\n",
    "     <li>Listo: La clasificacion puede indicar no solo '1' para cambio de estado sino un rango [-1,1] para indicar el sentido de dicho cambio</li>    \n",
    "</lu>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_fields = ['bid_price','ask_price' ,'event_price']\n",
    "int_fields = ['bid_size','ask_size','event_size']\n",
    "str_fields = ['event_type']\n",
    "date_fields = ['event_date','event_time']\n",
    "prices_and_sizes = reduce(lambda x,y : x+y , map(lambda x : [float_fields[x] , int_fields[x]] , [0,1,2]))\n",
    "colnames = date_fields + str_fields + prices_and_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparacion de datos y Generacion del set en dominio de frecuencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     20,
     40,
     51,
     82,
     100,
     104,
     107,
     115,
     131,
     138,
     160,
     258
    ]
   },
   "outputs": [],
   "source": [
    "def add_previous_offer(zs_tape):\n",
    "    prev_ask = []\n",
    "    prev_bid = []\n",
    "    bids = zs_tape['bid_price'].values\n",
    "    asks = zs_tape['ask_price'].values\n",
    "    zs_t = zs_tape.copy()\n",
    "    for i in range(0,len(zs_t)):\n",
    "        if i > 0 :\n",
    "            #print bids[i+1]\n",
    "            prev_bid.append(bids[i-1])\n",
    "            prev_ask.append(asks[i-1])\n",
    "        else:\n",
    "            #print \"NaN\"\n",
    "            prev_bid.append(\"NaN\")\n",
    "            prev_ask.append(\"NaN\")\n",
    "    zs_t['prev_bid'] = prev_bid\n",
    "    zs_t['prev_ask'] = prev_ask\n",
    "    \n",
    "    return zs_t\n",
    "\n",
    "def add_next_offer(zs_tape):\n",
    "    next_asks = []\n",
    "    next_bids = []\n",
    "    bids = zs_tape['bid_price'].values\n",
    "    asks = zs_tape['ask_price'].values\n",
    "    zs_t = zs_tape.copy()\n",
    "\n",
    "    for i in range(0,len(zs_t)):\n",
    "        if i < len(zs_t)-1:\n",
    "            #print bids[i+1]\n",
    "            next_bids.append(bids[i+1])\n",
    "            next_asks.append(asks[i+1])\n",
    "        else:\n",
    "            #print \"NaN\"\n",
    "            next_bids.append(\"NaN\")\n",
    "            next_asks.append(\"NaN\")\n",
    "    zs_t['next_bid'] = next_bids\n",
    "    zs_t['next_ask'] = next_asks\n",
    "    return zs_t\n",
    "\n",
    "def get_trade_at(x):\n",
    "    if x['event_type'] == 'TRADE':\n",
    "        if x['event_price'] == x['ask_price']:\n",
    "            return \"ASK\"\n",
    "        elif x['event_price'] == x['bid_price']:\n",
    "            return \"BID\"\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "    else:\n",
    "        return \"NaN\"\n",
    "\n",
    "def generate_trade_events(x):\n",
    "    if x['trade_at'] == 'BID':\n",
    "        #absorcion\n",
    "        if x['event_price'] == x['next_bid']:\n",
    "            return \"Absortion\"\n",
    "        elif x['next_bid'] < x['event_price']:\n",
    "            #ruptura\n",
    "            if x['event_size'] >= x['bid_size']:\n",
    "                return \"Break\" \n",
    "            #ruptura por retroceso\n",
    "            else:\n",
    "                return \"Break by Retracement\"\n",
    "        #absorcion por avance\n",
    "        else:\n",
    "            return \"Absortion by Advance\"\n",
    "    elif x['trade_at'] == 'ASK':\n",
    "        if x['event_price'] == x['next_ask']:\n",
    "            return \"Absortion\"\n",
    "        elif x['next_ask'] > x['event_price']:\n",
    "            #ruptura\n",
    "            if x['event_size'] >= x['ask_size']:\n",
    "                return \"Break\" \n",
    "            #ruptura por retroceso\n",
    "            else:\n",
    "                return \"Break by Retracement\"\n",
    "        #absorcion por avance\n",
    "        else:\n",
    "            return \"Absortion by Advance\"        \n",
    "    else:\n",
    "        return \"NaN\"\n",
    "    \n",
    "def generate_tick_events(x):\n",
    "    if x['event_type'] == 'ASK':\n",
    "        if x['event_price'] < x['next_ask']:\n",
    "            return 'Retracement'\n",
    "        elif x['event_price'] > x['next_ask']:\n",
    "            return 'Advance'\n",
    "        else:\n",
    "            return 'Place'\n",
    "    elif x['event_type'] == 'BID':\n",
    "        if x['event_price'] < x['next_bid']:\n",
    "            return 'Advance'\n",
    "        elif x['event_price'] > x['next_bid']:\n",
    "            return 'Retracement'\n",
    "        else:\n",
    "            return 'Place'        \n",
    "    else:\n",
    "        return \"NaN\"\n",
    "    \n",
    "def get_hour_and_minute(x):\n",
    "    tokens = x.split(':')\n",
    "    return (tokens[0],tokens[1])\n",
    "\n",
    "def get_intervals(h,m,window):\n",
    "    return [h +':'+ m + ':' + str(x) for x in range(0,60 , window)]\n",
    "\n",
    "def get_intervals_collection(frame,window):\n",
    "    hours_and_mins = set( map( lambda x: get_hour_and_minute(x) , frame['time'].values))\n",
    "    times = reduce(lambda x,y : x+y , [get_intervals(x[0],x[1],window) for x in hours_and_mins])\n",
    "    times_c = map(lambda x : datetime.strptime(x , '%H:%M:%S') , times)\n",
    "    all_intervals_i = sorted(times_c)\n",
    "    all_intervals_e = map(lambda x : datetime.strftime(x , '%H:%M:%S') , all_intervals_i)\n",
    "    return all_intervals_e\n",
    "\n",
    "def criteria(x,start,end):\n",
    "    \n",
    "    tstart = x[0]\n",
    "    tend = x[1]\n",
    "    label = x[2]\n",
    "    \n",
    "    start = datetime.strptime(start , '%H:%M:%S')\n",
    "    end = datetime.strptime(end , '%H:%M:%S')\n",
    "    tstart = datetime.strptime(tstart , '%H:%M:%S')\n",
    "    tend = datetime.strptime(tend , '%H:%M:%S')  \n",
    "    \n",
    "    if start >= tstart and end <= tend:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_label_for_interval(label_id , start , end , flabels):\n",
    "    t = filter(lambda x: criteria(x , start, end) , flabels)\n",
    "    if len(t) > 0:\n",
    "        return t[0][2]\n",
    "    else:\n",
    "        return \"NaN\"\n",
    "    \n",
    "def drop_na_and_clean(frame):\n",
    "    frame2 = frame.copy()\n",
    "    for c in colnames:\n",
    "        frame2 = frame2[ ~pd.isnull(frame2[c])]\n",
    "\n",
    "    #for x in str_fields + date_fields:\n",
    "    #    frame2[x] = frame2[x].apply(lambda x : x.strip())\n",
    "\n",
    "    for x in float_fields:\n",
    "        frame2[x] = frame2[x].apply(lambda x : float(x))\n",
    "    \n",
    "    ## le agrego el pm o am. Los datos entre las 10 y las 12 son AM, los demas son PM.\n",
    "    ##f = lambda x : x['event_time'] + ' am' if (x['event_time'] >= '10:00:00') & (x['event_time'] <= '12:00:00') else x['event_date'] + ' pm' \n",
    "    ##frame2['event_time'] = frame2.apply(f, axis=1)\n",
    "    ##frame2['datetime'] = frame2.apply(lambda x: datetime.strptime(x['event_date']+x['event_time'], '%Y-%m-%d%I:%M:%S.%f %p') , axis=1)\n",
    "    ##frame2['event_time'] = frame2.apply(lambda x : x['datetime'].strftime(\"%H:%M:%S.%f\") , axis=1 )\n",
    "    ##frame2 = frame2.set_index(['datetime'])\n",
    "\n",
    "    frame2 = clean_data(frame2)\n",
    "    \n",
    "    return frame2 \n",
    "\n",
    "def prepare_data_in_time_domain(frame):\n",
    "    frame2 = frame.copy()\n",
    "    ## Agregamos los anteriores Asks y Bids para cada registro\n",
    "    frame2 = add_previous_offer(frame2)\n",
    "    ## Agregamos los siguientes Bids y Asks para cada registro\n",
    "    frame2 = add_next_offer(frame2)\n",
    "\n",
    "    frame2['trade_at'] = frame2.apply(lambda x : get_trade_at(x) , axis = 1)\n",
    "    frame2['trade_event'] = frame2.apply( lambda x : generate_trade_events(x) ,axis = 1)\n",
    "    frame2['tick_event'] = frame2.apply( lambda x : generate_tick_events(x) ,axis = 1)\n",
    "    return frame2\n",
    "\n",
    "def generate_final_set(frame, max_limit , min_limit , func_datetimes ):\n",
    "    \n",
    "    trades_at_bid_size = []\n",
    "    trades_at_ask_size = []\n",
    "    trades_at_unknowns_size = []\n",
    "    trades_at_bid_count = []\n",
    "    trades_at_ask_count = []\n",
    "    trades_at_unknowns_count = []\n",
    "\n",
    "    initial_ask_prices = []\n",
    "    initial_bid_prices = []\n",
    "    final_ask_prices = []\n",
    "    final_bid_prices = []\n",
    "  \n",
    "    \n",
    "    starts = []\n",
    "    ends = []\n",
    "    \n",
    "    #datetimes = frame.apply(lambda x : datetime.strptime( x['date']+x['time'], '%Y-%m-%d%H:%M:%S.%f') , axis = 1)\n",
    "    #datetimes = map(lambda x : get_datetime_without_mili(x) , datetimes)\n",
    "    #datetimes = pd.unique(datetimes)\n",
    "    datetimes = func_datetimes(frame)\n",
    "    \n",
    "    for i in range(0 , len(datetimes)):\n",
    "\n",
    "        if i < len(datetimes) -1:\n",
    "\n",
    "            start = datetimes[i]\n",
    "            end = datetimes[i+1]\n",
    "\n",
    "            starts.append(start)\n",
    "            ends.append(end)\n",
    "            \n",
    "            mini_set = frame[ (frame.index >= start) & (frame.index < end) ]\n",
    "\n",
    "            ## Agrego el price en este momento\n",
    "            ##\n",
    "            initial_ask_price = mini_set['ask_price'].head(1).values[0]\n",
    "            initial_bid_price = mini_set['bid_price'].head(1).values[0]\n",
    "            last_ask_price = mini_set['ask_price'].tail(1).values[0]\n",
    "            last_bid_price = mini_set['bid_price'].tail(1).values[0]\n",
    "            initial_ask_prices.append(initial_ask_price)\n",
    "            initial_bid_prices.append(initial_bid_price)\n",
    "            final_ask_prices.append(last_ask_price)\n",
    "            final_bid_prices.append(last_bid_price)\n",
    "            \n",
    "            ## operaciones a precio de mkt\n",
    "            ##\n",
    "            trades_at_bid = mini_set[mini_set['trade_at'] == 'BID']\n",
    "            trades_at_ask = mini_set[mini_set['trade_at'] == 'ASK']\n",
    "            trades_at_unknown = mini_set[mini_set['trade_at'] == 'Unknown']\n",
    "\n",
    "            ## compras y ventas a mkt\n",
    "            ##\n",
    "            trades_at_bid_count.append(len(trades_at_bid))\n",
    "            trades_at_bid_size.append(sum(trades_at_bid['event_size']))\n",
    "            trades_at_ask_count.append(len(trades_at_ask))\n",
    "            trades_at_ask_size.append(sum(trades_at_ask['event_size']))\n",
    "            trades_at_unknowns_size.append(sum(trades_at_unknown['event_size']))\n",
    "            trades_at_unknowns_count.append(len(trades_at_unknown))\n",
    "\n",
    "           \n",
    "    obj = {'start':starts , 'end': ends,\n",
    "            'trades_at_bid_size': trades_at_bid_size , \n",
    "           'trades_at_ask_size': trades_at_ask_size , \n",
    "           'trades_at_unknowns_size': trades_at_unknowns_size, \n",
    "           'trades_at_bid_count': trades_at_bid_count , \n",
    "           'trades_at_ask_count': trades_at_ask_count,\n",
    "           'trades_at_unknowns_count': trades_at_unknowns_count , \n",
    "           \n",
    "           'initial_ask_price':initial_ask_prices ,\n",
    "           'initial_bid_price':initial_bid_prices ,\n",
    "           'final_ask_price':final_ask_prices ,\n",
    "           'final_bid_price':final_bid_prices }\n",
    "             \n",
    "    n_columns=['start' , 'end',\n",
    "        'trades_at_bid_size' , \n",
    "        'trades_at_bid_count' ,                \n",
    "        'trades_at_ask_size' , \n",
    "        'trades_at_ask_count',               \n",
    "        'trades_at_unknowns_size', \n",
    "        'trades_at_unknowns_count' , \n",
    "        'initial_ask_price','initial_bid_price','final_ask_price','final_bid_price'] \n",
    "    \n",
    "    return pd.DataFrame(obj , columns=n_columns) \n",
    "\n",
    "def process_data_and_generate_set(inputdata, metadata ):\n",
    "\n",
    "    min_limit = metadata.getMinDateTime()\n",
    "    max_limit = metadata.getMaxDateTime()\n",
    "    all_labels = metadata.intervals\n",
    "    output_name = metadata.outputName\n",
    "    \n",
    "    outputSet = None\n",
    "    \n",
    "    ## 1) preparamos la data en el dominio de tiempo\n",
    "    frame1 = prepare_data_in_time_domain(input_data)\n",
    "\n",
    "    ## 2) generamos el dataset en el dominio de frecuencias\n",
    "    frame2 = generate_final_set_2(frame1,max_limit , min_limit)\n",
    "\n",
    "    ## 3) stablish limits to set\n",
    "    frame3 = frame2[(frame2['start'] >= min_limit) & (frame2['end'] <= max_limit)]\n",
    "\n",
    "    ## 4) apply labels\n",
    "    frame4 = apply_labels(frame3, all_labels , output_name)\n",
    "    outputSet = frame4\n",
    "    \n",
    "    if metadata.requireAdvances:\n",
    "        ## 5) apply advances to labels\n",
    "        advance_levels = advance_levels = [.5 + float(x)/20 for x in range(0,10)]\n",
    "        frame5 = add_advance_in_frame_for_output_2(frame4 , all_labels , advance_levels , output_name)\n",
    "        outputSet = frame5\n",
    "    \n",
    "    return outputSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     1,
     16,
     19,
     31
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def _get_unique_datetimes_for_set_second_window(frame , window_seconds):\n",
    "    framex = frame.copy()\n",
    "\n",
    "    def get_window(x , size):\n",
    "        r = x / size\n",
    "        return r * size\n",
    "    \n",
    "    def get_trunked_datetime(d , t):\n",
    "        dd = datetime.strptime(d+t,'%Y-%m-%d%H:%M:%S.%f')\n",
    "        return datetime(dd.year , dd.month, dd.day , dd.hour, dd.minute, get_window(dd.second,window_seconds))\n",
    "\n",
    "    applied = framex.apply(lambda x : get_trunked_datetime(x.event_date ,  x.event_time) , axis=1) \n",
    "\n",
    "    return np.unique(applied)\n",
    "\n",
    "def get_unique_datetimes_by_5_seconds(frame):\n",
    "    return _get_unique_datetimes_for_set_second_window(frame,5)\n",
    "\n",
    "def get_unique_datetimes_by_1_minute(frame):\n",
    "    framex = frame.copy()\n",
    "\n",
    "    def get_trunked_datetime(d , t):\n",
    "        dd = datetime.strptime(d+t,'%Y-%m-%d%H:%M:%S.%f')\n",
    "        return datetime(dd.year , dd.month, dd.day , dd.hour, dd.minute, 0)\n",
    "\n",
    "\n",
    "    applied = framex.apply(lambda x : get_trunked_datetime(x.event_date ,  x.event_time) , axis=1) \n",
    "\n",
    "    return np.unique(applied)\n",
    "\n",
    "def get_unique_datetimes_by_1_second(frame):\n",
    "    framex = frame.copy()\n",
    "\n",
    "    def get_trunked_datetime(d , t):\n",
    "        dd = datetime.strptime(d+t,'%Y-%m-%d%H:%M:%S.%f')\n",
    "        return datetime(dd.year , dd.month, dd.day , dd.hour, dd.minute, dd.second)\n",
    "\n",
    "\n",
    "    applied = framex.apply(lambda x : get_trunked_datetime(x.event_date ,  x.event_time) , axis=1) \n",
    "\n",
    "    return np.unique(applied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Obtenemos los sets en dominio de frecuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenemos las metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_data_frame(metadata , func_datetimes_frequencies):\n",
    "    ## 2) leer la data de postgre\n",
    "    frame = get_frame_by_date_range(metadata.table , metadata.minDate , metadata.minTime , metadata.maxDate , metadata.maxTime)\n",
    "\n",
    "    ## 3) limpiamos la data\n",
    "    input_data = drop_na_and_clean(frame)\n",
    "\n",
    "    ## 1) preparamos la data en el dominio de tiempo\n",
    "    frame1 = prepare_data_in_time_domain(input_data)\n",
    "\n",
    "    ## 2) generamos el dataset en el dominio de frecuencias\n",
    "    frame2 = generate_final_set(frame1,metadata.getMaxDateTime() , metadata.getMinDateTime(),func_datetimes_frequencies)\n",
    "\n",
    "    ## 3) stablish limits to set\n",
    "    #frame3 = frame2[(frame2['start'] >= metadata.getMinDateTime()) & (frame2['end'] <= metadata.getMaxDateTime())]\n",
    "    \n",
    "    return frame2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "metadata_names = [\n",
    "    'nymex_future_gc_201712_neg_rev_alpha0_20171031_081200_20171031_165959',\n",
    "\n",
    "]\n",
    "\n",
    "metadatas = []\n",
    "for m in metadata_names:\n",
    "    metadatas.append( get_metadata(m ) )\n",
    "\n",
    "print len(metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171101_030000_20171101_165959',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171101_180000_20171102_165959',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171102_180000_20171103_165959',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171105_180000_20171106_080000',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171106_180000_20171107_165959',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171107_180000_20171108_165959',\n",
    "#   'nymex_future_gc_201712_neg_rev_alpha0_20171108_180000_20171109_165959',\n",
    "#    'nymex_future_gc_201712_neg_rev_alpha0_20171109_180000_20171110_165959'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generamos los sets por primera vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leidos: 88526\n",
      "Leidos: 88526\n",
      "Leidos: 88526\n"
     ]
    }
   ],
   "source": [
    "## usamos frequencias de 5 segundos\n",
    "\n",
    "initial_data_frames = {}\n",
    "\n",
    "for m in metadatas:\n",
    "    initial_data_frames['1_second_'+ m.labelName ] = get_data_frame(m , get_unique_datetimes_by_1_second)\n",
    "    initial_data_frames['5_second_' + m.labelName] = get_data_frame(m,get_unique_datetimes_by_5_seconds)\n",
    "    initial_data_frames['1_minute_' + m.labelName] = get_data_frame(m,get_unique_datetimes_by_1_minute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardamos en Pickle los Data Frames inciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando: 1_second_nymex_future_gc_201712_neg_rev_alpha0_20171031_081200_20171031_165959 con 26239 registros\n",
      "Comparando: 26239 vs 26239\n",
      "Guardando: 5_seconds_nymex_future_gc_201712_neg_rev_alpha0_20171031_081200_20171031_165959 con 6183 registros\n",
      "Comparando: 6183 vs 6183\n",
      "Guardando: 1_minute_nymex_future_gc_201712_neg_rev_alpha0_20171031_081200_20171031_165959 con 527 registros\n",
      "Comparando: 527 vs 527\n"
     ]
    }
   ],
   "source": [
    "for i in initial_data_frames.keys():\n",
    "    df = initial_data_frames[i]\n",
    "    print 'Guardando:',i,'con', len(df),'registros'\n",
    "    #put_mongo_data_frame(i + '_initial', df)\n",
    "    put_pickle('initial' , i , df)\n",
    "\n",
    "    dfx = get_pickle('initial' , i)\n",
    "    print 'Comparando:',len(df),'vs',len(dfx)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
